*ğŸ“˜ Agentic RAG Chatbot for Multi-Format Document QA using Model Context Protocol (MCP)*


**ğŸš€ Overview**
  
  This project implements a modular Retrieval-Augmented Generation (RAG) chatbot powered by an agent-based architecture and Model Context Protocol (MCP). Users can upload documents in various formats, ask complex questions, and receive context-rich answers backed by retrieved document content.

**ğŸ§  Core Features**

- âœ… Supports multi-format document ingestion: PDF, PPTX, DOCX, CSV, TXT/Markdown

- âœ… Agentic architecture with modular responsibilities

- âœ… MCP: In-memory message passing between agents

- âœ… Embedded vector search using FAISS/Chroma and OpenAI/HuggingFace embeddings

- âœ… Multi-turn QA with context-aware responses

- âœ… Web UI: Upload, chat, see sources


**ğŸ§± Architecture**

***Agentic Design (MCP-driven):***

```
- flowchart LR
- A[User Uploads File / Asks Query]
- A --> B(IngestionAgent)
- B --> C(RetrievalAgent)
- C --> D(LLMResponseAgent)
- D --> A
```


1. **IngestionAgent:** Parses uploaded files into clean text chunks.
2. **RetrievalAgent:** Converts query into embedding â†’ retrieves top-k matching chunks.
3. **LLMResponseAgent:** Forms prompt using query + chunks â†’ gets answer from LLM.
4. **MCP (Model Context Protocol):** Message bus used by agents to pass structured context and results.

**ğŸ§¾ Message Format (MCP)**
```
{
  "sender": "RetrievalAgent",
  "receiver": "LLMResponseAgent",
  "type": "RETRIEVED_CONTEXT",
  "trace_id": "abc-123",
  "payload": {
    "top_chunks": ["..."],
    "query": "What are the KPIs?"
  }
}
```

**âš™ï¸ Tech Stack**

| **Layer**           | **Technology**                     |
|---------------------|------------------------------------|
| Frontend (UI)       | Streamlit / React                  |
| Backend Agents      | Python                             |
| Protocol            | In-memory Bus (MCP)                |
| Embeddings          | OpenAI / HuggingFace               |
| Vector Store        | FAISS / Chroma                     |
| Document Parsing    | PyMuPDF, python-docx, etc.         |

**ğŸ“‚ File Structure**

```YAML
.
â”œâ”€â”€ app/                # Main UI & server
â”œâ”€â”€ core/               # Common utilities (MCP, VectorDB)
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ ingestion.py    # IngestionAgent
â”‚   â”œâ”€â”€ retrieval.py    # RetrievalAgent
â”‚   â””â”€â”€ llm_response.py # LLMResponseAgent
â”œâ”€â”€ data/               # Uploaded/parsed files
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ run.py              # Entry point
```


ğŸ§ª Setup Instructions

```
# Clone repository
git clone https://github.com/yourusername/agentic-rag-chatbot.git
cd agentic-rag-chatbot

# Install dependencies
python -m venv venv && source venv/bin/activate
pip install -r requirements.txt

# Run the app
python run.py
```


**ğŸ–¥ï¸ Usage**

1. Upload supported files (PDF, DOCX, CSV, etc.)

2. Ask questions in natural language

3. View generated answers + retrieved context chunks

**ğŸ§  Sample Use Case**

```
Q: "Summarize the KPIs mentioned in the Q2 report."
A: "The KPIs mentioned are revenue growth (12%), customer retention (87%), and average deal size..."
```


**ğŸ“Œ Future Enhancements**
 
- Add LangChain Agent interface

- Replace in-memory bus with Kafka/NATS

- PDF highlighting with source

- Advanced UI chat history

**ğŸ¤ Contributing**

Contributions are welcome. Fork the repo, make changes, and open a PR.


**âš–ï¸ License**

MIT License. See LICENSE file for details.
